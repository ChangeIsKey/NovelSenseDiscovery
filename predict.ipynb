{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChangeIsKey/NovelSenseDiscovery/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD5AYXb4Z9mm"
      },
      "outputs": [],
      "source": [
        "# ares\n",
        "#!wget http://sensembert.org/resources/ares_embedding.tar.gz\n",
        "#!tar -xf ares_embedding.tar.gz\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg-sYdz6c4gW"
      },
      "outputs": [],
      "source": [
        "# sensebert sample\n",
        "#!rm -r NovelSenseDiscovery\n",
        "!git clone https://github.com/ChangeIsKey/NovelSenseDiscovery\n",
        "\n",
        "#!cd NovelSenseDiscovery/data/ && tar -xf sensembert_data.tar.gz\n",
        "#!ls NovelSenseDiscovery/data/sensembert_data \n",
        "#sense_vectors = 'NovelSenseDiscovery/data/sensembert_data/sensembert_EN_supervised.txt'\n",
        "\n",
        "!cd NovelSenseDiscovery/data/ && tar -xf ares_embedding.tar.gz\n",
        "!ls NovelSenseDiscovery/data/ares_embedding\n",
        "#sense_vectors = 'NovelSenseDiscovery/data/ares_embedding/ares_bert_large_sample.txt'\n",
        "sense_vectors = 'ares_embedding/ares_bert_large.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3ttRcrrvhgM"
      },
      "outputs": [],
      "source": [
        "# get discovery repo\n",
        "#!rm -r LSCDiscovery\n",
        "!git clone https://github.com/seinan9/LSCDiscovery\n",
        "!pip install -r LSCDiscovery/requirements.txt\n",
        "!python -m spacy download en_core_web_sm\n",
        "#!pip install --force-reinstall -v \"spacy==3.1.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neIRUAsM-S48"
      },
      "outputs": [],
      "source": [
        "#load vectors\n",
        "#!git clone https://github.com/Garrafao/LSCDetection\n",
        "'''\n",
        "from LSCDiscovery.modules.utils_ import Space\n",
        "\n",
        "# Load matrices\n",
        "try: \n",
        "    space = Space(sense_vectors, format='npz') \n",
        "except ValueError: \n",
        "    space = Space(sense_vectors, format='w2v')\n",
        "\n",
        "sense_vecs = space.matrix.toarray() #much more efficient\n",
        "#split vectors\n",
        "sense_vecs = sense_vecs[:,:1024] #use synset embedding\n",
        "#sense_vecs = sense_vecs[:,1024:] #use sense embedding\n",
        "row2id_senses = space.row2id\n",
        "id2row_senses = space.id2row\n",
        "\n",
        "#display(sense_vecs)\n",
        "#display(row2id_senses)\n",
        "#display(sense_vecs.shape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from gensim.models import KeyedVectors\n",
        "sense_vecs = KeyedVectors.load_word2vec_format(sense_vectors, binary=False)\n",
        "row2id_senses = {k:i for (i,k) in enumerate(sense_vecs.vocab.keys())}\n",
        "id2row_senses = {i:k for (i,k) in enumerate(sense_vecs.vocab.keys())}\n",
        "#display(id2row_senses)\n",
        "vecs = []\n",
        "for i in sense_vecs.vocab.keys():\n",
        "    vecs.append(sense_vecs.get_vector(i))\n",
        "sense_vecs = numpy.array(vecs) #much more efficient\n",
        "\n",
        "#split vectors\n",
        "sense_vecs = sense_vecs[:,:1024] #use synset embedding\n",
        "#sense_vecs = sense_vecs[:,1024:] #use sense embedding"
      ],
      "metadata": {
        "id": "mA1jaTZGdAaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TClU8-bgmIN5"
      },
      "outputs": [],
      "source": [
        "# get benchmark\n",
        "#!git clone https://github.com/ChangeIsKey/LSCDBenchmark\n",
        "#!pip install -r LSCDBenchmark/requirements.txt\n",
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zbg-okeoXXA"
      },
      "outputs": [],
      "source": [
        "#import sys \n",
        "#sys.path.append('./LSCDBenchmark/') \n",
        "#import main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpaF2xGOwxTc"
      },
      "outputs": [],
      "source": [
        "# download semeval corpora\n",
        "#!cd LSCDiscovery && bash scripts/import_semeval_en.sh\n",
        "#!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg8ekZ_TyUHS"
      },
      "outputs": [],
      "source": [
        "# sample uses\n",
        "#!cd LSCDiscovery && bash -e scripts/prepare_sample.sh en_semeval sample_1 2 10 en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yganSxVz6DUJ"
      },
      "outputs": [],
      "source": [
        "#intersect sense and corpus vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyXn15wWKVN1"
      },
      "outputs": [],
      "source": [
        "# extract usage vectors \n",
        "#!cd LSCDiscovery && bash scripts/discover_bert.sh en_semeval sample_1 en-large toklem 1+12 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsZaKNY9TUUL"
      },
      "outputs": [],
      "source": [
        "#extract wsd data\n",
        "!cd NovelSenseDiscovery/data/ && tar -xf data_files.tar.gz\n",
        "#!ls NovelSenseDiscovery/data/data_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpIs_dR3_k5M"
      },
      "outputs": [],
      "source": [
        "# visualize targets\n",
        "#!ls LSCDiscovery/data/en_semeval/samples/sample_1\n",
        "#targets_path = 'LSCDiscovery/data/en_semeval/samples/sample_1/sample.tsv'\n",
        "targets_path = 'NovelSenseDiscovery/data/all.tsv'\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(targets_path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile,fieldnames=['target'], delimiter='\\t',quoting=csv.QUOTE_NONE,strict=True)\n",
        "    targets = [row['target'] for row in reader]\n",
        "\n",
        "#display(targets)\n",
        "\n",
        "#uses_path = 'LSCDiscovery/data/en_semeval/samples/sample_1/usages_corpus1/casket.tsv'\n",
        "#uses_path = 'NovelSenseDiscovery/data/data_files/1970s.tsv'\n",
        "#!ls LSCDiscovery/data/en_semeval/samples/sample_1/usages_corpus1\n",
        "# Uses\n",
        "#with open(uses_path, encoding='utf-8') as csvfile:\n",
        "#    reader = csv.DictReader(csvfile, delimiter='\\t',quoting=csv.QUOTE_NONE,strict=True)\n",
        "#    rows = [row for row in reader]\n",
        "#display(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv2QaLEB4uqC"
      },
      "outputs": [],
      "source": [
        "#vectorize wsd test usages\n",
        "'''\n",
        "%%bash\n",
        "files=NovelSenseDiscovery/data/data_files\n",
        "outputdir=NovelSenseDiscovery/data/data_files_vectors\n",
        "rm -r $outputdir\n",
        "mkdir $outputdir\n",
        "cd LSCDiscovery\n",
        "i=0\n",
        "for file in ../$files/*.tsv\n",
        "do\n",
        "    echo $file\n",
        "    python contextualized/bert.py -l $file ../$outputdir/$(basename \"${file%.*}\") en-large toklem 1+12\n",
        "    if [ $i -eq 20 ];\n",
        "    then\n",
        "        break\n",
        "    fi\n",
        "    i=$(( $i + 1 ))\n",
        "    #break\n",
        "done\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "sys.path.append('LSCDiscovery/modules/')\n",
        "import csv\n",
        "import gzip\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
        "from utils_ import Space\n",
        "\n",
        "file_folder = 'NovelSenseDiscovery/data/data_files'\n",
        "output_folder = 'NovelSenseDiscovery/data/data_files_vectors'\n",
        "language = 'en-large'\n",
        "type_sentences = 'toklem'\n",
        "layers = '1+12'\n",
        "\n",
        "is_len = True\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary) and model (weights)\n",
        "model_language = {\n",
        "        'en-large': 'bert-large-cased', \n",
        "        'en': 'bert-base-cased', \n",
        "        'de': 'bert-base-german-cased', \n",
        "        'sw': 'KB/bert-base-swedish-cased', \n",
        "        'multi': 'bert-base-multilingual-cased'\n",
        "        }\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_language[language])\n",
        "model = BertModel.from_pretrained(model_language[language], output_hidden_states=True)\n",
        "\n",
        "if type_sentences == 'toklem':\n",
        "    type_ = 'token'\n",
        "else:\n",
        "    type_ = type_sentences\n",
        "\n",
        "for target in targets[:20]:\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    display(target)\n",
        "\n",
        "    path_usages = file_folder+'/'+target+'.tsv'\n",
        "    path_output = output_folder+'/'+target\n",
        "    \n",
        "    # Load sentences \n",
        "    context_vector_list = []\n",
        "    test_sentences = []\n",
        "    with open(path_usages, 'r') as csvFile:\n",
        "        reader = csv.DictReader(csvFile, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, strict=True)\n",
        "        for row in reader:\n",
        "            test_sentences.append(dict(row))\n",
        "\n",
        "        # Create the vectors\n",
        "        for i in range(0, len(test_sentences)):\n",
        "            try:\n",
        "                # Create target word(s)\n",
        "                target_words = []\n",
        "                target_word = str(test_sentences[i][\"sentence_\"+type_].split()[int([test_sentences[i][\"index_\"+type_]][0])])\n",
        "                if type_sentences == 'toklem':\n",
        "                    original_word = test_sentences[i][\"lemma\"]\n",
        "                    target_words.append(tokenizer.tokenize(original_word))\n",
        "                else:\n",
        "                    clean_target_word = \"\".join(char for k,char in enumerate(target_word) if char.isalpha() or char == \"-\" or (char == \"'\" and k == len(target_word)-1))\n",
        "                    if clean_target_word[-1] == \"'\":\n",
        "                        clean_target_word = test_sentences[i][\"lemma\"]\n",
        "                    target_words.append(tokenizer.tokenize(clean_target_word))\n",
        "                target_words = target_words[0]\n",
        "                \n",
        "                # Tokenize text\n",
        "                text = test_sentences[i][\"sentence_\"+type_]\n",
        "                if type_sentences == 'toklem':\n",
        "                    text = text.replace(target_word, original_word) \n",
        "                marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "                tokenized_text = tokenizer.tokenize(marked_text)\n",
        "            \n",
        "                # Search the indices of the tokenized target word in the tokenized text\n",
        "                target_word_indices = []\n",
        "                for j in range(0, len(tokenized_text)):\n",
        "                    if tokenized_text[j] == target_words[0]:\n",
        "                        for l in range(0, len(target_words)):\n",
        "                            if tokenized_text[j+l] == target_words[l]:\n",
        "                                target_word_indices.append(j+l)\n",
        "                            if len(target_word_indices) == len(target_words):\n",
        "                                break\n",
        "                \n",
        "                if len(target_word_indices) == 0:\n",
        "                    logging.info(\"INDICES NOT FOUND. SKIPPED SENTENCE \"+str(i))\n",
        "                    continue\n",
        "\n",
        "                # Trim tokenized_text if longer than 512\n",
        "                if len(tokenized_text) > 512:\n",
        "                    while (len(tokenized_text) > 512):\n",
        "                        if tokenized_text[-1] != tokenized_text[target_word_indices[-1]]:\n",
        "                            del(tokenized_text[-1])\n",
        "                        else:\n",
        "                            del(tokenized_text[0])\n",
        "                            for index, value in enumerate(target_word_indices):\n",
        "                                target_word_indices[index] -= 1\n",
        "                \n",
        "                # Create BERT Token Embeddings\n",
        "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "                segments_ids = [1] * len(tokenized_text)\n",
        "                tokens_tensor = torch.tensor([indexed_tokens])\n",
        "                segments_tensors = torch.tensor([segments_ids])\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(tokens_tensor, segments_tensors)\n",
        "                    hidden_states = outputs[2]\n",
        "                token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "                token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "                token_embeddings = token_embeddings.permute(1, 0, 2)\n",
        "                vectors = []\n",
        "                for number in target_word_indices:\n",
        "                    token = token_embeddings[number]\n",
        "                    layers_list = layers.split('+')\n",
        "                    layers_list = list(map(int, layers_list))\n",
        "                    #vec = [np.array(token[l]) for l in layers_list]\n",
        "                    sum_vec = np.sum([np.array(token[l]) for l in layers_list], axis=0)\n",
        "                    #sum_vec = np.sum([np.array(token[12]), np.array(token[1])], axis=0)\n",
        "                    vectors.append(np.array(sum_vec))\n",
        "                context_vector_list.append(np.sum(vectors, axis=0, dtype=float))\n",
        "            except:\n",
        "                print(\"SKIPPED SENTENCE \"+str(i))\n",
        "        \n",
        "    \n",
        "    # Normalize vectors in length\n",
        "    if is_len:\n",
        "        context_vector_list = preprocessing.normalize(context_vector_list, norm='l2')\n",
        "\n",
        "    # Save contextVectorList_sparse matrix\n",
        "    rows = [row['instance_id'] if 'instance_id' in row else str(i) for i, row in enumerate(test_sentences)]\n",
        "    outSpace = Space(matrix=context_vector_list, rows=rows, columns=[])\n",
        "    outSpace.save(path_output)\n",
        "\n",
        "    logging.info(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "ccU5NpBM1YX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S06Qkkctl9h"
      },
      "outputs": [],
      "source": [
        "#load vectors for each target and assign nearest neighbor sense\n",
        "#!ls LSCDiscovery/output/en_semeval/BERT_layers1+12_typetoklem/discovery/sample_1/t0.1/vectors_corpus1\n",
        "from LSCDiscovery.modules.utils_ import Space\n",
        "import numpy\n",
        "import csv\n",
        "from sklearn.neighbors import NearestNeighbors \n",
        "#vecs_path = 'LSCDiscovery/output/en_semeval/BERT_layers1+12_typetoklem/discovery/sample_1/t0.1/vectors1'\n",
        "#vecs_path = 'LSCDiscovery/output/en_semeval/BERT_layers1+12_typetoklem/discovery/sample_1/t0.1/vectors2'\n",
        "vecs_path = 'NovelSenseDiscovery/data/data_files_vectors'\n",
        "#output_file = 'predictions_coha.csv'\n",
        "output_file = 'predictions_data_files.csv'\n",
        "usage2sensepred = {}\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='cosine').fit(sense_vecs)\n",
        "for target in targets:\n",
        "        \n",
        "    path_matrix1 = vecs_path+'/'+target\n",
        "    \n",
        "    # Load matrices\n",
        "    try: \n",
        "        try:\n",
        "            space1 = Space(path_matrix1, format='npz') \n",
        "        except ValueError: \n",
        "            space1 = Space(path_matrix1, format='w2v') \n",
        "    except FileNotFoundError: \n",
        "        continue\n",
        "\n",
        "    display(target)\n",
        "\n",
        "    row2id1 = space1.row2id\n",
        "    id2row1 = space1.id2row       \n",
        "    #display(id2row1)\n",
        "    vectors1 = space1.matrix.toarray() \n",
        "    #display(vectors1)\n",
        "    #usage_vecs_no = vectors1.shape[0]\n",
        "    #vecs = numpy.vstack((vectors1,sense_vecs))\n",
        "    #vecs_no = vecs.shape[0]\n",
        "    #row2id = {}\n",
        "    #for (row,id) in row2id1 | row2id2 | row2id_sense:\n",
        "    #    row2id[row] = row\n",
        "\n",
        "    #Get nearest neighbor sense\n",
        "    distances, indices = nbrs.kneighbors(vectors1) \n",
        "    #display(indices)\n",
        "    for i, r in id2row1.items():\n",
        "         usage2sensepred[target+'_'+str(i)] = id2row_senses[indices[i][0]] #todo add usage id\n",
        "\n",
        "display(usage2sensepred)\n",
        "\n",
        "predictions = [{'usage_id': u, 'sense_id':s} for (u,s) in usage2sensepred.items()]\n",
        "# Export predictions\n",
        "with open(output_file, 'w') as f:  \n",
        "    w = csv.DictWriter(f, predictions[0].keys(), delimiter='\\t', quoting = csv.QUOTE_NONE, quotechar='')\n",
        "    w.writeheader()\n",
        "    w.writerows(predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIH88Um6/G0ciT8Oxe29w2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}