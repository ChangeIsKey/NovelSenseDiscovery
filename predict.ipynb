{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4LH0bzHTSUUQCkE72X5c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChangeIsKey/NovelSenseDiscovery/blob/main/load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD5AYXb4Z9mm"
      },
      "outputs": [],
      "source": [
        "# ares\n",
        "import requests\n",
        "url =\"http://sensembert.org/resources/ares_embedding.tar.gz\" \n",
        "r = requests.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sensebert sample\n",
        "!rm -r NovelSenseDiscovery\n",
        "!git clone https://github.com/ChangeIsKey/NovelSenseDiscovery\n",
        "\n",
        "#!cd NovelSenseDiscovery/data/ && tar -xf sensembert_data.tar.gz\n",
        "#!ls NovelSenseDiscovery/data/sensembert_data \n",
        "#sense_vectors = 'NovelSenseDiscovery/data/sensembert_data/sensembert_EN_supervised.txt'\n",
        "\n",
        "!cd NovelSenseDiscovery/data/ && tar -xf ares_embedding.tar.gz\n",
        "!ls NovelSenseDiscovery/data/ares_embedding\n",
        "sense_vectors = 'NovelSenseDiscovery/data/ares_embedding/ares_bert_base_multilingual.txt'"
      ],
      "metadata": {
        "id": "Xg-sYdz6c4gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get discovery repo\n",
        "!rm -r LSCDiscovery\n",
        "!git clone https://github.com/seinan9/LSCDiscovery\n",
        "!pip install -r LSCDiscovery/requirements.txt\n",
        "!python -m spacy download en_core_web_sm\n",
        "#!pip install --force-reinstall -v \"spacy==3.1.1\""
      ],
      "metadata": {
        "id": "T3ttRcrrvhgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load vectors\n",
        "#!git clone https://github.com/Garrafao/LSCDetection\n",
        "#from gensim.models import KeyedVectors\n",
        "#sense_vecs = KeyedVectors.load_word2vec_format(sense_vectors, binary=False)\n",
        "from LSCDiscovery.modules.utils_ import Space\n",
        "\n",
        "# Load matrices\n",
        "try: \n",
        "    space = Space(sense_vectors, format='npz') \n",
        "except ValueError: \n",
        "    space = Space(sense_vectors, format='w2v')\n",
        "\n",
        "sense_vecs = space.matrix.toarray()\n",
        "#split vectors\n",
        "sense_vecs = sense_vecs[:,:1024] #use synset embedding\n",
        "#sense_vecs = sense_vecs[1024:] #use sense embedding\n",
        "row2id_senses = space.row2id\n",
        "id2row_senses = space.id2row\n",
        "\n",
        "#display(sense_vecs)\n",
        "#display(row2id_senses)\n",
        "#display(sense_vecs.shape)"
      ],
      "metadata": {
        "id": "neIRUAsM-S48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get benchmark\n",
        "#!git clone https://github.com/ChangeIsKey/LSCDBenchmark\n",
        "#!pip install -r LSCDBenchmark/requirements.txt\n",
        "#!pip install torch"
      ],
      "metadata": {
        "id": "TClU8-bgmIN5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import sys \n",
        "#sys.path.append('./LSCDBenchmark/') \n",
        "#import main"
      ],
      "metadata": {
        "id": "8zbg-okeoXXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download semeval corpora\n",
        "!cd LSCDiscovery && bash scripts/import_semeval_en.sh\n",
        "#!ls data"
      ],
      "metadata": {
        "id": "fpaF2xGOwxTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample uses\n",
        "!cd LSCDiscovery && bash -e scripts/prepare_sample.sh en_semeval sample_1 10 25 en"
      ],
      "metadata": {
        "id": "Sg8ekZ_TyUHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intersect sense and corpus vocab"
      ],
      "metadata": {
        "id": "yganSxVz6DUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract usage vectors \n",
        "!cd LSCDiscovery && bash scripts/discover_bert.sh en_semeval sample_1 en-large toklem 1+12 0.1"
      ],
      "metadata": {
        "id": "IyXn15wWKVN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract wsd data\n",
        "!cd NovelSenseDiscovery/data/ && tar -xf data_files.tar.gz\n",
        "#!ls NovelSenseDiscovery/data/data_files"
      ],
      "metadata": {
        "id": "LsZaKNY9TUUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize targets\n",
        "#!ls LSCDiscovery/data/en_semeval/samples/sample_1\n",
        "#targets_path = 'LSCDiscovery/data/en_semeval/samples/sample_1/sample.tsv'\n",
        "targets_path = 'NovelSenseDiscovery/data/all.tsv'\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(targets_path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile,fieldnames=['target'], delimiter='\\t',quoting=csv.QUOTE_NONE,strict=True)\n",
        "    targets = [row['target'] for row in reader]\n",
        "\n",
        "#display(targets)\n",
        "\n",
        "#uses_path = 'LSCDiscovery/data/en_semeval/samples/sample_1/usages_corpus1/casket.tsv'\n",
        "#uses_path = 'NovelSenseDiscovery/data/data_files/1970s.tsv'\n",
        "#!ls LSCDiscovery/data/en_semeval/samples/sample_1/usages_corpus1\n",
        "# Uses\n",
        "#with open(uses_path, encoding='utf-8') as csvfile:\n",
        "#    reader = csv.DictReader(csvfile, delimiter='\\t',quoting=csv.QUOTE_NONE,strict=True)\n",
        "#    rows = [row for row in reader]\n",
        "#display(rows)"
      ],
      "metadata": {
        "id": "jpIs_dR3_k5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorize wsd test usages\n",
        "%%bash\n",
        "files=NovelSenseDiscovery/data/data_files\n",
        "outputdir=NovelSenseDiscovery/data/data_files_vectors\n",
        "rm -r $outputdir\n",
        "mkdir $outputdir\n",
        "cd LSCDiscovery\n",
        "for file in ../$files/*.tsv\n",
        "do\n",
        "    echo $file\n",
        "    python contextualized/bert.py -l $file ../$outputdir/$(basename \"${file%.*}\") en-large toklem 1+12\n",
        "    #break #for test\n",
        "done"
      ],
      "metadata": {
        "id": "Rv2QaLEB4uqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge"
      ],
      "metadata": {
        "id": "vUJ-EURq4nX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load vectors for each target and assign nearest neighbor sense\n",
        "#!ls LSCDiscovery/output/en_semeval/BERT_layers1+12_typetoklem/discovery/sample_1/t0.1/vectors_corpus1\n",
        "\n",
        "import numpy\n",
        "import csv\n",
        "from sklearn.neighbors import NearestNeighbors \n",
        "#vecs_path = 'LSCDiscovery/output/en_semeval/BERT_layers1+12_typetoklem/discovery/sample_1/t0.1/vectors1'\n",
        "#vecs_path = 'LSCDiscovery/output/en_semeval/BERT_layers1+12_typetoklem/discovery/sample_1/t0.1/vectors2'\n",
        "vecs_path = 'NovelSenseDiscovery/data/data_files_vectors'\n",
        "#output_file = 'predictions_coha.csv'\n",
        "output_file = 'predictions_data_files.csv'\n",
        "usage2sensepred = {}\n",
        "for target in targets:\n",
        "    display(target)\n",
        "        \n",
        "    path_matrix1 = vecs_path+'/'+target\n",
        "    \n",
        "    # Load matrices\n",
        "    try:          \n",
        "        try: \n",
        "            space1 = Space(path_matrix1, format='npz') \n",
        "        except ValueError:\n",
        "            space1 = Space(path_matrix1, format='w2v') \n",
        "    except FileNotFoundError:        \n",
        "         continue\n",
        "\n",
        "    row2id1 = space1.row2id\n",
        "\n",
        "    #display(row2id1)\n",
        "\n",
        "    vectors1 = space1.matrix.toarray() \n",
        "    usage_vecs_no = vectors1.shape[0]\n",
        "    vecs = numpy.vstack((vectors1,sense_vecs))\n",
        "    vecs_no = vecs.shape[0]\n",
        "    #row2id = {}\n",
        "    #for (row,id) in row2id1 | row2id2 | row2id_sense:\n",
        "    #    row2id[row] = row\n",
        "\n",
        "    #Get nearest neighbor sense\n",
        "    nbrs = NearestNeighbors(n_neighbors=vecs_no, algorithm='auto', metric='cosine').fit(vecs) \n",
        "    distances, indices = nbrs.kneighbors(vecs) \n",
        "    #display(indices)\n",
        "    for i in range(indices.shape[0]):\n",
        "        inds = indices[i]\n",
        "        for nbr in inds:\n",
        "            if nbr > usage_vecs_no: #todo validate\n",
        "                usage2sensepred[target+'_'+str(i-usage_vecs_no)] = id2row_senses[nbr-usage_vecs_no] #todo add usage id\n",
        "                break\n",
        "        #break #shorten list for test \n",
        "\n",
        "display(usage2sensepred)\n",
        "\n",
        "predictions = [{'usage_id': u, 'sense_id':s} for (u,s) in usage2sensepred.items()]\n",
        "# Export predictions\n",
        "with open(output_file, 'w') as f:  \n",
        "    w = csv.DictWriter(f, predictions[0].keys(), delimiter='\\t', quoting = csv.QUOTE_NONE, quotechar='')\n",
        "    w.writeheader()\n",
        "    w.writerows(predictions)"
      ],
      "metadata": {
        "id": "0S06Qkkctl9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
